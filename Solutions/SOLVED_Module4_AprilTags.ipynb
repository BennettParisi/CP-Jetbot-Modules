{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: AprilTags\n",
    "\n",
    "Our camera is now able to detect images and perform basic modifications to the frame captured. With this, we will now use a library to detect specific markers known as \"AprilTags\". They are similar to QR codes, in that they contain information using black and white squares on a 2D image (such as a paper), but are much smaller, to improve detectability and robustness.\n",
    "\n",
    "This module should follow Module 3: Camera and Computer Vision Basics\n",
    "\n",
    "Here are the normal packages we need to import in our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip3 install --upgrade pip\n",
    "# pip3 install dt-apriltags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot, Camera, bgr8_to_jpeg\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display, Image, clear_output, Markdown\n",
    "import ipywidgets as widgets\n",
    "import time\n",
    "\n",
    "robot = Robot()\n",
    "camera = Camera.instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need a new one, known as dt_apriltags! Additionally, we must create a \"detector\" object that will be used to analyze the image taken from the camera to find any april tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dt_apriltags\n",
    "\n",
    "detector = dt_apriltags.Detector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following loop will detect AprilTags located in the frame of the camera, draw a box around the tag detected, and display the AprilTag's ID. The individual lines will be explored in the block after the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection Loop\n",
    "while True:\n",
    "    clear_output(wait = True)\n",
    "    image = np.array(camera.value)\n",
    "    gray_frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    detections = detector.detect(gray_frame)\n",
    "\n",
    "    for detection in detections:\n",
    "        corners = detection.corners\n",
    "        corners = [(int(x), int(y)) for (x, y) in corners]\n",
    "\n",
    "        cv2.polylines(image, [np.array(corners)], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        tag_id = detection.tag_id\n",
    "        cv2.putText(image, f\"ID: {tag_id}\", (int(corners[0][0]), int(corners[0][1])-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    \n",
    "    display(Image(data=bgr8_to_jpeg(image)))\n",
    "    time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give you a sense of the various code in this block, here is an explanation walking through each line.\n",
    "\n",
    "<i>\n",
    "<code>clear_output(wait = True)</code>\n",
    "\n",
    "<code>image = np.array(camera.value)</code>\n",
    "\n",
    "<code>gray_frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</code>\n",
    "</i>\n",
    "\n",
    "The first few lines are similar to what we accomplished in Module 3. We take the image captured by the camera and convert it to a grayscale image.\n",
    "\n",
    "<i><code>detections = detector.detect(gray_frame)</i></code>\n",
    "\n",
    "This line is the main purpose of the AprilTags library we imported. The \"detector\", which we created above when we first imported the package, is an object/class. The \"detect\" is a method within this class that takes an image and returns a list of april tags detected in the image. Each AprilTag is stored in its own custom format, with features that define both what it is and where it is (this will be shown later).\n",
    "\n",
    "<i><code>for detection in detections:</i></code>\n",
    "\n",
    "Since the return is a list of AprilTags detected, we can go through each detected AprilTag in this for loop.\n",
    "\n",
    "<i><code>corners = detection.corners</i></code>\n",
    "\n",
    "<i><code>corners = [(int(x), int(y)) for (x, y) in corners]</i></code>\n",
    "\n",
    "One of the specific ways an AprilTag is stored is with locating where the four corners are within the image frame. These two lines save those corner values, and then convert them into integers (just for ease of imaging). It is important to note that they are stored as purely x and y coordinates within the frame of the image, and not in the actual world. Therefore, these values refer to the specific pixels of the image for their location.\n",
    "\n",
    "<i><code>cv2.polylines(image, [np.array(corners)], isClosed=True, color=(0, 255, 0), thickness=2)</i></code>\n",
    "\n",
    "This line uses cv2 to create a box that can be seen visually when we look at the image within our jupyter notebook.\n",
    "\n",
    "<i><code>tag_id = detection.tag_id</i></code>\n",
    "\n",
    "<i><code>cv2.putText(image, f\"ID: {tag_id}\", (int(corners[0][0]), int(corners[0][1])-10),\n",
    "    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)</i></code>\n",
    "\n",
    "This is one of the other pieces of data stored within the AprilTag. Because there are different patterns that an AprilTag can encompass, they each have their own unique ID to denote which pattern is which. If you try different AprilTags, you will notice they all have their own unique IDs that show on the image. The function cv2.putText is used to display these values on the image so we can see visually what tag it is finding.\n",
    "\n",
    "<i><code>display(Image(data=bgr8_to_jpeg(image)))</i></code>\n",
    "\n",
    "<i><code>time.sleep(0.05)</i></code>\n",
    "\n",
    "Same as in Module 3, this converts the image to jpeg and displays, while the time.sleep sets the framerate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Tag Pose\n",
    "\n",
    "Now we want to determine where that AprilTag is located with respect to the camera. To accomplish this, we need two sets of data: our camera's intrinsic parameters and the size of the AprilTag (in meters). Luckily we already have the intrinsic parameters from Module 3, we just need to measure the size of the AprilTag.\n",
    "\n",
    "Once you get your data, enter it into the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update these values below\n",
    "cam_mtx = [94.861, 119.511, 119.506, 114.969]\n",
    "AT_size = .065"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the same code as above, but with a slight tweak. When using the detect method, as in:\n",
    "\n",
    "> detections = detector.detect(gray_frame)\n",
    "\n",
    "there are other parameters that can be inputted that are by default set to Nones. But these will allow us to actually detect the position or pose of the AprilTag. From the dt_apriltags library documentation, the function header for the detect method appears as so:\n",
    "\n",
    "> def detect(self, img, estimate_tag_pose=False, camera_params=None, tag_size=None):\n",
    "\n",
    "To estimate the tag_pose, we must set the parameter to \"True\", set the camera_params to the intrinsic parameters we entered above, and the tag_size to the measurements we made. Nowwe can see the translation and rotation matrices which tell us the position and orientation of the AprilTag in relation to the camera (and if we know the world position and orientation of the AprilTag, we can find the position and orientation of the Jetbot itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Widgets for displaying output\n",
    "image_widget = widgets.Image(format='jpeg')\n",
    "tag_label = widgets.Label()\n",
    "pose_t_output = widgets.Output()\n",
    "pose_R_output = widgets.Output()\n",
    "\n",
    "display(widgets.HBox([image_widget, tag_label, pose_t_output, pose_R_output]))\n",
    "\n",
    "while True:\n",
    "    clear_output(wait = True)\n",
    "    image = np.array(camera.value)\n",
    "    gray_frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # TODO: Write this line with the necessary parameters from the function header shown above\n",
    "    detections = detector.detect(gray_frame, True, cam_mtx, AT_size)\n",
    "\n",
    "    for detection in detections:\n",
    "        corners = detection.corners\n",
    "        corners = [(int(x), int(y)) for (x, y) in corners]\n",
    "\n",
    "        cv2.polylines(image, [np.array(corners)], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        tag_id = detection.tag_id\n",
    "        cv2.putText(image, f\"ID: {tag_id}\", (int(corners[0][0]), int(corners[0][1])-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    \n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "\n",
    "    # Getting Pose Values from AprilTag\n",
    "    if detections:\n",
    "        tag_label.value = f\"**Last Detected ID:** {tag_id}\"\n",
    "        pose_t_str = np.array2string(detections[0].pose_t, precision=3, suppress_small=True)\n",
    "        pose_R_str = np.array2string(detections[0].pose_R, precision=3, suppress_small=True)\n",
    "        with pose_t_output:\n",
    "            pose_t_output.clear_output(wait=True)\n",
    "            print(\"Translation Pose (t):\")\n",
    "            print(pose_t_str)\n",
    "        with pose_R_output:\n",
    "            pose_R_output.clear_output(wait=True)\n",
    "            print(\"Rotation Pose (R):\")\n",
    "            print(pose_R_str)\n",
    "        \n",
    "    time.sleep(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above this, you should see the camera feed from the <b><span style=\"color:#154734\">JetBot</span></b>, as well as the Tag ID and the matrices for both translation and rotation. The translation matrix should be in the form of [x, y, z].\n",
    "\n",
    "Now using a ruler, place the AprilTag 10cm away from the front of the camera, and record the z value as determined by the detector. Repeat these steps again for 20cm and 30cm as well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
