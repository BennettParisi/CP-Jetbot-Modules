{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: AprilTags\n",
    "\n",
    "Our camera is now able to detect images and perform basic modifications to the frame captured. With this, we will now use a library to detect specific markers known as \"AprilTags\". They are similar to QR codes, in that they contain information using black and white squares on a 2D image (such as a paper), but are much smaller, to improve detectability and robustness.\n",
    "\n",
    "This module should follow Module 3: Camera and Computer Vision Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the normal packages we need to import in our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot, Camera, bgr8_to_jpeg\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display, Image, clear_output\n",
    "import time\n",
    "\n",
    "robot = Robot()\n",
    "camera = Camera.instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a new one, known as duckietown april tags! We create a \"detector\" that will be used to analyze the image taken from the camera to find any april tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dt_apriltags\n",
    "\n",
    "detector = dt_apriltags.Detector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following loop will detect AprilTags located in the frame of the camera, draw a box around the tag detected, and display the apriltag (and its ID). The individual lines will be explored in the block after the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    clear_output(wait = True)\n",
    "    image = np.array(camera.value)\n",
    "    gray_frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    detections = detector.detect(gray_frame)\n",
    "\n",
    "    for detection in detections:\n",
    "        corners = detection.corners\n",
    "        corners = [(int(x), int(y)) for (x, y) in corners]\n",
    "\n",
    "        cv2.polylines(image, [np.array(corners)], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        tag_id = detection.tag_id\n",
    "        cv2.putText(image, f\"ID: {tag_id}\", (int(corners[0][0]), int(corners[0][1])-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    \n",
    "    display(Image(data=bgr8_to_jpeg(image)))\n",
    "    time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clear_output(wait = True)\n",
    "image = np.array(camera.value)\n",
    "gray_frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "The first few lines are similar to what we accomplished in Module 3. We take the image captured by the camera and convert it to a grayscale image.\n",
    "\n",
    "detections = detector.detect(gray_frame)\n",
    "\n",
    "This line is the main purpose of the april tags library we imported. The \"detector\", which we created above when we first imported the package, is an object/class. The \"detect\" is a method within this class that takes an image and returns a list of april tags detected in the image. Each april tag is stored in its own custom format, with features that define both what it is and where it is (this will be shown later).\n",
    "\n",
    "for detection in detections:\n",
    "\n",
    "Since the return is a list of april tags detected, we can go through each detected april tag in this for loop.\n",
    "\n",
    "corners = detection.corners\n",
    "corners = [(int(x), int(y)) for (x, y) in corners]\n",
    "\n",
    "One of the specific ways an apriltag is stored is with locating where the four corners are within the image frame. These two lines save those corner values, and then convert them into integers (just for ease of imaging). It is important to note that they are stored as purely x and y coordinates within the frame of the image, and not in the actual world. Therefore, these values refer to the specific pixels of the image for their location.\n",
    "\n",
    "cv2.polylines(image, [np.array(corners)], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "This line uses cv2 to create a box that can be seen visually when we look at the image within our jupyter notebook.\n",
    "\n",
    "tag_id = detection.tag_id\n",
    "cv2.putText(image, f\"ID: {tag_id}\", (int(corners[0][0]), int(corners[0][1])-10),\n",
    "    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "This is one of the other pieces of data stored within the april tag. Because there are different patterns that an AprilTag can encompass, they each have their own unique ID to denote which pattern is which. If you try different AprilTags, you will notice they all have their own unique IDs that show on the image. The function cv2.putText is used to display these values on the image so we can see visually what tag it is finding.\n",
    "\n",
    "display(Image(data=bgr8_to_jpeg(image)))\n",
    "time.sleep(0.05)\n",
    "\n",
    "Same as in Module 3, this converts the image to jpeg and displays, while the time.sleep sets the framerate."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
