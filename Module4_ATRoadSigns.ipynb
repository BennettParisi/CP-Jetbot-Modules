{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 4: AprilTags\n",
    "\n",
    "Our camera is now able to detect images and perform basic modifications to the frame captured. With this, we will now use a library to detect specific markers known as \"AprilTags\". They are similar to QR codes, in that they contain information using black and white squares on a 2D image (such as a paper), but are much smaller, to improve detectability and robustness.\n",
    "\n",
    "This module should follow Module 3: Camera and Computer Vision Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the normal packages we need to import in our code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot, Camera, bgr8_to_jpeg\n",
    "import cv2\n",
    "import numpy as np\n",
    "from IPython.display import display, Image, clear_output\n",
    "import time\n",
    "\n",
    "robot = Robot()\n",
    "camera = Camera.instance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a new one, known as duckietown april tags! We create a \"detector\" that will be used to analyze the image taken from the camera to find any april tags. But first, we must check if the Jetbot has the duckietowns april tag library. If we open a terminal within jetbot, we can run the following command to see the list of installed packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python3 -m pip install --upgrade pip\n",
    "\n",
    "python3 -m pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are looking for the package labeled dt-apriltags. If it is not there, we will need to run the following command to install it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python3 -m pip install dt-apriltags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dt_apriltags\n",
    "\n",
    "detector = dt_apriltags.Detector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This following loop will detect AprilTags located in the frame of the camera, draw a box around the tag detected, and display the apriltag (and its ID). The individual lines will be explored in the block after the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    clear_output(wait = True)\n",
    "    image = np.array(camera.value)\n",
    "    gray_frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    detections = detector.detect(gray_frame)\n",
    "\n",
    "    for detection in detections:\n",
    "        corners = detection.corners\n",
    "        corners = [(int(x), int(y)) for (x, y) in corners]\n",
    "\n",
    "        cv2.polylines(image, [np.array(corners)], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        tag_id = detection.tag_id\n",
    "        cv2.putText(image, f\"ID: {tag_id}\", (int(corners[0][0]), int(corners[0][1])-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    \n",
    "    display(Image(data=bgr8_to_jpeg(image)))\n",
    "    time.sleep(0.05)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "clear_output(wait = True)\n",
    "image = np.array(camera.value)\n",
    "gray_frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "The first few lines are similar to what we accomplished in Module 3. We take the image captured by the camera and convert it to a grayscale image.\n",
    "\n",
    "detections = detector.detect(gray_frame)\n",
    "\n",
    "This line is the main purpose of the april tags library we imported. The \"detector\", which we created above when we first imported the package, is an object/class. The \"detect\" is a method within this class that takes an image and returns a list of april tags detected in the image. Each april tag is stored in its own custom format, with features that define both what it is and where it is (this will be shown later).\n",
    "\n",
    "for detection in detections:\n",
    "\n",
    "Since the return is a list of april tags detected, we can go through each detected april tag in this for loop.\n",
    "\n",
    "corners = detection.corners\n",
    "corners = [(int(x), int(y)) for (x, y) in corners]\n",
    "\n",
    "One of the specific ways an apriltag is stored is with locating where the four corners are within the image frame. These two lines save those corner values, and then convert them into integers (just for ease of imaging). It is important to note that they are stored as purely x and y coordinates within the frame of the image, and not in the actual world. Therefore, these values refer to the specific pixels of the image for their location.\n",
    "\n",
    "cv2.polylines(image, [np.array(corners)], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "This line uses cv2 to create a box that can be seen visually when we look at the image within our jupyter notebook.\n",
    "\n",
    "tag_id = detection.tag_id\n",
    "cv2.putText(image, f\"ID: {tag_id}\", (int(corners[0][0]), int(corners[0][1])-10),\n",
    "    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "\n",
    "This is one of the other pieces of data stored within the april tag. Because there are different patterns that an AprilTag can encompass, they each have their own unique ID to denote which pattern is which. If you try different AprilTags, you will notice they all have their own unique IDs that show on the image. The function cv2.putText is used to display these values on the image so we can see visually what tag it is finding.\n",
    "\n",
    "display(Image(data=bgr8_to_jpeg(image)))\n",
    "time.sleep(0.05)\n",
    "\n",
    "Same as in Module 3, this converts the image to jpeg and displays, while the time.sleep sets the framerate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating Tag Pose\n",
    "\n",
    "Now we want to determine where that AprilTag is located with respect to the camera. To accomplish this, we need two sets of data: our camera's intrinsic parameters and the size of the AprilTag (in meters). Luckily we already have the intrinsic parameters from Module 3, we just need to measure the size of the AprilTag.\n",
    "\n",
    "Once you get your data, enter it into the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "Don't forget to remove the values below! - Bennett\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_mtx = [94.861, 119.511, 119.506, 114.969]\n",
    "AT_size = .065"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the same code as above, but with a slight tweak. When using the detect method, as in:\n",
    "\n",
    "detections = detector.detect(gray_frame)\n",
    "\n",
    "there are other parameters that can be inputted that are by default set to Nones. But these will allow us to actually detect the position or pose of the AprilTag. From the dt_apriltags library documentation, the function header for the detect method appears as so:\n",
    "\n",
    "def detect(self, img, estimate_tag_pose=False, camera_params=None, tag_size=None):\n",
    "\n",
    "To estimate the tag_pose, we must set the parameter to \"True\", set the camera_params to the intrinsic parameters we entered above, and the tag_size to the measurements we made. Nowwe can see the translation and rotation matrices which tell us the position and orientation of the AprilTag in relation to the camera (and if we know the world position and orientation of the AprilTag, we can find the position and orientation of the Jetbot itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    clear_output(wait = True)\n",
    "    image = np.array(camera.value)\n",
    "    gray_frame = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    detections = detector.detect(gray_frame, True, cam_mtx, AT_size)\n",
    "\n",
    "    for detection in detections:\n",
    "        corners = detection.corners\n",
    "        corners = [(int(x), int(y)) for (x, y) in corners]\n",
    "\n",
    "        cv2.polylines(image, [np.array(corners)], isClosed=True, color=(0, 255, 0), thickness=2)\n",
    "\n",
    "        tag_id = detection.tag_id\n",
    "        cv2.putText(image, f\"ID: {tag_id}\", (int(corners[0][0]), int(corners[0][1])-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
    "    \n",
    "    display(Image(data=bgr8_to_jpeg(image)))\n",
    "    if detections:\n",
    "        display(f\"ID: {detections[0].tag_id}\")\n",
    "        display(f\"Translation Pose: {detections[0].pose_t}\")\n",
    "        display(f\"Rotation Pose: {detections[0].pose_R}\")\n",
    "    time.sleep(0.05)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
